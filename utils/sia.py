# utils/sia_idt.py
from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, Any, Optional, Tuple, Literal, List
import numpy as np


@dataclass
class SIAResult:
    pred_cid: int
    losses: Dict[int, float]  # cid -> loss (lower is better)
    probs: Optional[Dict[int, float]]  # softmax(-loss/T)
    confidence: float  # é¢„æµ‹ç½®ä¿¡åº¦
    num_ties: int  # å¹³å±€æ•°é‡


@dataclass
class SIAMetrics:
    """
    SIA æ ¸å¿ƒæŒ‡æ ‡ï¼ˆç²¾ç®€ç‰ˆï¼‰

    ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡ï¼š
    1. ASR: æ”»å‡»æˆåŠŸç‡
    2. Confidence: å¹³å‡é¢„æµ‹ç½®ä¿¡åº¦
    3. Tie Rate: å¹³å±€ç‡
    """
    asr: float
    confidence: float
    tie_rate: float
    num_clients: int
    num_samples: int
    random_baseline: float

    def __str__(self) -> str:
        if self.tie_rate > 0.3 or self.confidence < self.random_baseline * 2:
            privacy_level = "HIGH ğŸ›¡ï¸"
        elif self.tie_rate > 0.1 or self.confidence < 0.5:
            privacy_level = "MEDIUM ğŸ”’"
        else:
            privacy_level = "LOW âš ï¸"

        return (
            f"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n"
            f"â•‘        SIA Attack Metrics (IDT)           â•‘\n"
            f"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n"
            f"â•‘ ASR:               {self.asr:6.2%}                   â•‘\n"
            f"â•‘ Random Baseline:   {self.random_baseline:6.2%}                   â•‘\n"
            f"â•‘ Advantage:         {self.asr / self.random_baseline:6.2f}x                  â•‘\n"
            f"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n"
            f"â•‘ Confidence:        {self.confidence:6.2%}                   â•‘\n"
            f"â•‘ Tie Rate:          {self.tie_rate:6.2%}                   â•‘\n"
            f"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n"
            f"â•‘ Privacy Level: {privacy_level:26s} â•‘\n"
            f"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n"
            f"â•‘ Samples: {self.num_samples:5d}  |  Clients: {self.num_clients:2d}        â•‘\n"
            f"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        )

    def to_dict(self) -> Dict[str, float]:
        return {
            'asr': self.asr,
            'confidence': self.confidence,
            'tie_rate': self.tie_rate,
            'random_baseline': self.random_baseline,
            'advantage': self.asr / self.random_baseline,
        }


LossMode = Literal["cross_entropy", "margin_loss"]


class SourceInferenceAttackerIDT:
    """
    åŸºäºè®ºæ–‡çš„æ ·æœ¬çº§æºæ¨æ–­æ”»å‡»ï¼ˆIDT ç‰ˆæœ¬ï¼‰

    æ ¸å¿ƒåŸç†ï¼ˆå¯¹åº”è®ºæ–‡ Theorem 2ï¼‰:
        pred_cid = argmin_k â„“(Î¸_k, z)
        å³ï¼šé¢„æµ‹æŸå¤±æœ€å°çš„å®¢æˆ·ç«¯æœ€å¯èƒ½æ‹¥æœ‰è¯¥æ ·æœ¬

    loss_mode:
      - "cross_entropy": -log p(y|x)  [æ¨èï¼Œå¯¹åº”è®ºæ–‡]
      - "margin_loss": -[p(y|x) - max_{câ‰ y} p(c|x)]
    """

    def __init__(
            self,
            loss_mode: LossMode = "cross_entropy",
            temperature: float = 1.0,
            use_probs: bool = True,
            print_ties: bool = False,
            eps: float = 1e-12,
            seed: int = 42,
    ):
        self.seed = int(seed)
        self._rng = np.random.default_rng(self.seed)
        self.loss_mode = loss_mode
        self.temperature = float(temperature)
        self.use_probs = bool(use_probs)
        self.print_ties = bool(print_ties)
        self.eps = float(eps)

    # -------------------------
    # æ•°æ®è½¬æ¢è¾…åŠ©å‡½æ•°
    # -------------------------
    def _to_single_graph_batch(self, data_or_batch: Any) -> Any:
        """å°†å•ä¸ªå›¾è½¬æ¢ä¸º Batch"""
        try:
            from torch_geometric.data import Batch
        except Exception as e:
            raise RuntimeError("éœ€è¦ torch_geometric") from e

        if hasattr(data_or_batch, "to_data_list"):
            dl = data_or_batch.to_data_list()
            if len(dl) != 1:
                raise ValueError(f"æœŸæœ›å•å›¾è¾“å…¥ï¼Œä½†å¾—åˆ° {len(dl)} ä¸ªå›¾")
            return Batch.from_data_list(dl)

        return Batch.from_data_list([data_or_batch])

    def _iter_graphs(self, data_or_batch: Any) -> List[Any]:
        """ä» Batch æˆ–å•ä¸ª Data ä¸­æå–å›¾åˆ—è¡¨"""
        if hasattr(data_or_batch, "to_data_list"):
            return list(data_or_batch.to_data_list())
        return [data_or_batch]

    # -------------------------
    # æ ¸å¿ƒï¼šæŸå¤±è®¡ç®—
    # -------------------------
    def _extract_label_single(self, batch_1: Any) -> Optional[int]:
        """æå–å•å›¾æ ‡ç­¾"""
        if not hasattr(batch_1, "y") or batch_1.y is None:
            return None
        y = batch_1.y
        if hasattr(y, "detach"):
            y = y.detach().cpu().numpy()
        y = np.asarray(y).reshape(-1)
        if y.size == 0:
            return None
        return int(y[0])

    def _get_classes_mapping(self, idt: Any) -> Optional[Dict[int, int]]:
        """è·å–ç±»åˆ«åˆ° predict_proba åˆ—ç´¢å¼•çš„æ˜ å°„"""
        try:
            dt = idt.out_layer.dt
            classes = getattr(dt, "classes_", None)
            if classes is None:
                return None
            classes = np.asarray(classes).tolist()
            return {int(c): i for i, c in enumerate(classes)}
        except Exception:
            return None

    def _ensure_proba_2d(self, proba: Any) -> np.ndarray:
        """ç¡®ä¿ proba æ˜¯ 2D æ•°ç»„"""
        p = np.asarray(proba, dtype=np.float64)
        if p.ndim == 1:
            p = np.stack([1.0 - p, p], axis=1)
        return p

    def _compute_loss_single(self, idt: Any, batch_1: Any) -> float:
        """
        è®¡ç®—å•ä¸ªæ ·æœ¬çš„é¢„æµ‹æŸå¤±ï¼ˆå¯¹åº”è®ºæ–‡ä¸­çš„ â„“(Î¸_k, z)ï¼‰

        è¿”å›å€¼è¶Šå° => è¯¥å®¢æˆ·ç«¯è¶Šå¯èƒ½æ‹¥æœ‰æ­¤æ ·æœ¬
        """
        if not hasattr(idt, "predict_proba"):
            raise AttributeError("éœ€è¦æ¨¡å‹å®ç° predict_proba(batch)")

        # è·å–é¢„æµ‹æ¦‚ç‡ (1, C)
        proba = self._ensure_proba_2d(idt.predict_proba(batch_1))
        if proba.shape[0] != 1:
            raise ValueError(f"æœŸæœ›å•æ ·æœ¬é¢„æµ‹æ¦‚ç‡ (1,C)ï¼Œå¾—åˆ° {proba.shape}")

        # è·å–çœŸå®æ ‡ç­¾
        y = self._extract_label_single(batch_1)
        if y is None:
            raise ValueError("SIA éœ€è¦çœŸå®æ ‡ç­¾æ¥è®¡ç®—æŸå¤±")

        # å¤„ç†ç±»åˆ«æ˜ å°„ï¼ˆsklearn å¯èƒ½åªè®­ç»ƒäº†éƒ¨åˆ†ç±»åˆ«ï¼‰
        cls2col = self._get_classes_mapping(idt)
        if cls2col is None:
            yy = int(np.clip(y, 0, proba.shape[1] - 1))
        else:
            yy = cls2col.get(int(y), None)
            if yy is None:
                return float('inf')

        # è®¡ç®—æŸå¤±
        if self.loss_mode == "cross_entropy":
            p_true = np.clip(proba[0, yy], self.eps, 1.0)
            return float(-np.log(p_true))

        elif self.loss_mode == "margin_loss":
            p = proba[0].copy()
            p_true = float(p[yy])
            p[yy] = -np.inf
            p_second = float(np.max(p)) if np.isfinite(np.max(p)) else 0.0
            margin = p_true - p_second
            return float(-margin)

        else:
            raise ValueError(f"æœªçŸ¥ loss_mode: {self.loss_mode}")

    # -------------------------
    # å…¬å…± API
    # -------------------------
    def infer_one_sample(self, idts_by_cid: Dict[int, Any], target_data: Any) -> SIAResult:
        """
        å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œæºæ¨æ–­

        Args:
            idts_by_cid: {client_id: IDT_model} å­—å…¸
            target_data: PyG Dataï¼ˆå•å›¾ï¼‰æˆ–åŒ…å«ä¸€ä¸ªå›¾çš„ Batch

        Returns:
            SIAResult(pred_cid, losses, probs, confidence, num_ties)
        """
        losses: Dict[int, float] = {}

        batch_1 = self._to_single_graph_batch(target_data)
        batch_1 = batch_1.to("cpu") if hasattr(batch_1, "to") else batch_1

        # è®¡ç®—æ¯ä¸ªå®¢æˆ·ç«¯æ¨¡å‹çš„æŸå¤±
        for cid, idt in idts_by_cid.items():
            losses[cid] = self._compute_loss_single(idt, batch_1)

        # é€‰æ‹©æŸå¤±æœ€å°çš„å®¢æˆ·ç«¯
        min_loss = min(losses.values())
        ties = [cid for cid, v in losses.items() if np.isclose(v, min_loss, rtol=1e-12, atol=1e-12)]

        if len(ties) == 1:
            pred_cid = ties[0]

            if self.print_ties:
                # éå¹³å±€ï¼šæ‰“å°æ‰€æœ‰å®¢æˆ·ç«¯çš„ loss
                print("[SIA][Unique-Min] per-client losses:")
                for cid in sorted(losses.keys()):
                    mark = "  <-- min" if cid == pred_cid else ""
                    print(f"    client {cid:2d}: loss = {losses[cid]:.6f}{mark}")

        else:
            pred_cid = int(self._rng.choice(ties))
            if self.print_ties:
                print(
                    f"[SIA][Tie] {len(ties)} clients share min loss {min_loss:.6f}: "
                    f"{sorted(ties)} -> randomly pick {pred_cid} (seed={self.seed})"
                )

        # è®¡ç®—åéªŒæ¦‚ç‡åˆ†å¸ƒ
        probs = None
        if self.use_probs:
            cids = sorted(losses.keys())
            L = np.array([losses[c] for c in cids], dtype=np.float32)
            logits = -L / max(self.temperature, 1e-8)
            exps = np.exp(logits - logits.max())
            p = exps / (exps.sum() + 1e-12)
            probs = {c: float(p[i]) for i, c in enumerate(cids)}

        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = probs[pred_cid] if probs else (1.0 / len(idts_by_cid))

        return SIAResult(
            pred_cid=pred_cid,
            losses=losses,
            probs=probs,
            confidence=confidence,
            num_ties=len(ties),
        )

    def eval_asr(
            self,
            idts_by_cid: Dict[int, Any],
            targets_by_true_cid: Dict[int, Any],
    ) -> Tuple[SIAMetrics, Dict[int, List[SIAResult]]]:
        """
        è¯„ä¼°æ”»å‡»æˆåŠŸç‡å¹¶è¿”å›3ä¸ªæ ¸å¿ƒæŒ‡æ ‡

        Args:
            idts_by_cid: {client_id: IDT_model}
            targets_by_true_cid: {true_client_id: Data/Batch/List[Data]}

        Returns:
            (metrics, results)
            - metrics: SIAMetrics å¯¹è±¡
            - results: {true_cid: [SIAResult, ...]}
        """
        correct, total = 0, 0
        results: Dict[int, List[SIAResult]] = {}

        all_confidences = []
        all_num_ties = []

        for true_cid, data_or_batch in targets_by_true_cid.items():
            graphs = self._iter_graphs(data_or_batch)
            per_client_results: List[SIAResult] = []

            for g in graphs:
                res = self.infer_one_sample(idts_by_cid, g)
                per_client_results.append(res)
                total += 1
                if res.pred_cid == true_cid:
                    correct += 1

                all_confidences.append(res.confidence)
                all_num_ties.append(res.num_ties)

            results[true_cid] = per_client_results

        # è®¡ç®—3ä¸ªæ ¸å¿ƒæŒ‡æ ‡
        num_clients = len(idts_by_cid)
        asr = correct / max(total, 1)
        mean_confidence = float(np.mean(all_confidences))
        tie_rate = float(np.mean([1 if n > 1 else 0 for n in all_num_ties]))

        metrics = SIAMetrics(
            asr=asr,
            confidence=mean_confidence,
            tie_rate=tie_rate,
            num_clients=num_clients,
            num_samples=total,
            random_baseline=1.0 / num_clients,
        )

        return metrics, results

